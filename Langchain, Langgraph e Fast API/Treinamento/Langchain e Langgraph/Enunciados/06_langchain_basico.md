# LangChain - Introdu√ß√£o e Conceitos B√°sicos

## O que √© LangChain?

LangChain √© um framework para construir aplica√ß√µes com Large Language Models (LLMs). Ele fornece abstra√ß√µes e ferramentas para:
- Conectar LLMs a fontes de dados
- Criar chains (cadeias) de processamento
- Gerenciar mem√≥ria e contexto
- Integrar com ferramentas externas

## Por que LangChain?

- üîó **Modularidade**: Componentes reutiliz√°veis
- üîÑ **Chains**: Conectar m√∫ltiplas opera√ß√µes
- üíæ **Mem√≥ria**: Manter contexto entre intera√ß√µes
- üõ†Ô∏è **Ferramentas**: Integrar com APIs, bancos de dados, etc.

## Exerc√≠cio 1: Primeira Conex√£o com LLM

### Objetivo
Conectar-se a um modelo LLM usando LangChain.

### Pr√©-requisitos

```bash
pip install langchain langchain-openai python-dotenv
```

Crie um arquivo `.env`:
```
OPENAI_API_KEY=sua-chave-aqui
```

### C√≥digo Base

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

load_dotenv()

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

response = llm.invoke("Qual √© a capital do Brasil?")
print(response.content)
```

### Exerc√≠cio Pr√°tico

1. Crie uma fun√ß√£o que recebe uma pergunta e retorna a resposta
2. Teste com diferentes perguntas
3. Experimente diferentes valores de temperature (0.0, 0.7, 1.0)

### Solu√ß√£o Esperada

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

load_dotenv()

def perguntar_llm(pergunta: str, temperatura: float = 0.7) -> str:
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=temperatura)
    response = llm.invoke(pergunta)
    return response.content

# Testes
print(perguntar_llm("Qual √© a capital do Brasil?", temperatura=0.0))
print(perguntar_llm("Conte uma piada", temperatura=1.0))
print(perguntar_llm("Explique quantum computing", temperatura=0.7))
```

## Exerc√≠cio 2: Trabalhando com Mensagens

### Objetivo
Entender como LangChain estrutura mensagens (System, Human, AI).

### C√≥digo Base

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

llm = ChatOpenAI()

messages = [
    SystemMessage(content="Voc√™ √© um assistente √∫til."),
    HumanMessage(content="Ol√°!")
]

response = llm.invoke(messages)
print(response.content)
```

### Exerc√≠cio Pr√°tico

1. Crie um assistente especializado em culin√°ria
2. Fa√ßa uma conversa com m√∫ltiplas mensagens
3. Mantenha o contexto da conversa

### Solu√ß√£o Esperada

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

llm = ChatOpenAI()

messages = [
    SystemMessage(content="Voc√™ √© um chef de cozinha experiente. Ajude com receitas e dicas culin√°rias."),
    HumanMessage(content="Como fa√ßo um bolo de chocolate?"),
]

response = llm.invoke(messages)
print("Chef:", response.content)

# Continuar a conversa
messages.append(AIMessage(content=response.content))
messages.append(HumanMessage(content="E se eu quiser fazer sem a√ß√∫car?"))

response2 = llm.invoke(messages)
print("Chef:", response2.content)
```

## Exerc√≠cio 3: Extraindo Informa√ß√µes Estruturadas

### Objetivo
Fazer o LLM retornar dados estruturados.

### Exerc√≠cio Pr√°tico

Crie uma fun√ß√£o que extrai informa√ß√µes de um texto:
- Nome
- Email
- Telefone
- Endere√ßo

### Solu√ß√£o Esperada

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import json

llm = ChatOpenAI(model="gpt-3.5-turbo")

def extrair_informacoes(texto: str) -> dict:
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Voc√™ √© um extrator de informa√ß√µes. Retorne apenas JSON v√°lido."),
        ("human", """Extraia as seguintes informa√ß√µes do texto abaixo e retorne em formato JSON:
        - nome
        - email
        - telefone
        - endereco
        
        Texto: {texto}""")
    ])
    
    chain = prompt | llm
    response = chain.invoke({"texto": texto})
    
    try:
        return json.loads(response.content)
    except:
        return {"erro": "N√£o foi poss√≠vel extrair informa√ß√µes"}

texto = """
Meu nome √© Jo√£o Silva, meu email √© joao@email.com,
telefone: (11) 99999-8888, e moro na Rua das Flores, 123, S√£o Paulo.
"""

info = extrair_informacoes(texto)
print(json.dumps(info, indent=2, ensure_ascii=False))
```

## Exerc√≠cio 4: Stream de Respostas

### Objetivo
Aprender a receber respostas em tempo real (streaming).

### C√≥digo Base

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(streaming=True)

for chunk in llm.stream("Conte uma hist√≥ria curta"):
    print(chunk.content, end="", flush=True)
```

### Exerc√≠cio Pr√°tico

Crie uma fun√ß√£o que:
1. Faz streaming da resposta
2. Processa cada chunk
3. Retorna a resposta completa

### Solu√ß√£o Esperada

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

def perguntar_com_stream(pergunta: str):
    llm = ChatOpenAI(streaming=True)
    prompt = ChatPromptTemplate.from_messages([
        ("human", "{pergunta}")
    ])
    
    chain = prompt | llm
    
    resposta_completa = ""
    print("Assistente: ", end="", flush=True)
    
    for chunk in chain.stream({"pergunta": pergunta}):
        if chunk.content:
            print(chunk.content, end="", flush=True)
            resposta_completa += chunk.content
    
    print()  # Nova linha
    return resposta_completa

resposta = perguntar_com_stream("Explique o que √© intelig√™ncia artificial em 3 par√°grafos")
```

## Exerc√≠cio 5: Tratamento de Erros

### Objetivo
Implementar tratamento robusto de erros com LangChain.

### Exerc√≠cio Pr√°tico

Crie uma fun√ß√£o que:
1. Tenta fazer uma requisi√ß√£o ao LLM
2. Trata erros de API
3. Retorna uma mensagem amig√°vel em caso de erro
4. Implementa retry autom√°tico

### Solu√ß√£o Esperada

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import time
from openai import APIError

def perguntar_com_retry(pergunta: str, max_tentativas: int = 3) -> str:
    llm = ChatOpenAI()
    prompt = ChatPromptTemplate.from_messages([
        ("human", "{pergunta}")
    ])
    
    chain = prompt | llm
    
    for tentativa in range(max_tentativas):
        try:
            response = chain.invoke({"pergunta": pergunta})
            return response.content
        except APIError as e:
            if tentativa == max_tentativas - 1:
                return f"Erro ao processar sua pergunta. Detalhes: {str(e)}"
            print(f"Tentativa {tentativa + 1} falhou. Tentando novamente...")
            time.sleep(2 ** tentativa)  # Backoff exponencial
        except Exception as e:
            return f"Erro inesperado: {str(e)}"
    
    return "N√£o foi poss√≠vel obter resposta ap√≥s v√°rias tentativas."

print(perguntar_com_retry("Qual √© a capital da Fran√ßa?"))
```

## Desafio Final

Crie um assistente de linha de comando que:
- Mant√©m hist√≥rico de conversa
- Permite diferentes "personas" (cientista, poeta, etc.)
- Faz streaming das respostas
- Trata erros graciosamente
- Salva o hist√≥rico em arquivo

## Pr√≥ximo Passo

Avan√ßar para **07_langchain_prompts.md** para dominar a cria√ß√£o de prompts eficazes.

